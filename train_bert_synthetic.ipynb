{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f589830f-9bf9-4a1b-8d09-a735b8d6d0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BfBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    ")\n",
    "from typing import List, Tuple\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "import brunoflow as bf\n",
    "from brunoflow.ad import Node\n",
    "from brunoflow.net import LogReg\n",
    "from brunoflow.opt import Adam, cross_entropy_loss, regularize\n",
    "from preprocessing.datasets import (\n",
    "    MNIST,\n",
    "    FFOOM,\n",
    "    NoisyLinear,\n",
    "    BreastCancer,\n",
    "    Bank7,\n",
    "    BankFull,\n",
    "    FirstTokenRepeatedOnce,\n",
    "    Contains1FirstToken,\n",
    "    ContainsTokenSet,\n",
    "    FirstTokenRepeatedOnceImmediately,\n",
    "    Contains1,\n",
    "    FirstTokenRepeatedImmediately,\n",
    "    FirstTokenRepeatedLast,\n",
    "    AdjacentDuplicate,\n",
    "    BinCountOnes,\n",
    ")\n",
    "\n",
    "from utils import catchtime, gpu_memory_usage\n",
    "from kvq_utils import (\n",
    "    convert_sentence_to_tokens_and_target_idx,\n",
    "    find_matching_nodes,\n",
    "    preprocessgrad_per_parent_per_word_data_per_layer,\n",
    "    rename_matmul_kvq_nodes,\n",
    "    summarize_max_grad_kvq,\n",
    "    plot_max_grad_against_layer_per_word,\n",
    ")\n",
    "from entropy_utils import (\n",
    "    gather_entropies_of_input_ids,\n",
    "    gather_abs_grad_of_input_ids,\n",
    "    gather_grad_of_input_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d08b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running JAX on cpu\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import jax\n",
    "    from jax import numpy as jnp\n",
    "\n",
    "    jax_device_kind = jax.devices()[0].device_kind\n",
    "    print(f\"Running JAX on {jax_device_kind}\")\n",
    "    # if \"NVIDIA\" not in jax_device_kind:\n",
    "    #     raise ValueError(\"Imported JAX, but not running on a GPU, terminating.\")\n",
    "except ImportError:\n",
    "    print(\"No JAX available to import!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cb83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.join(\n",
    "    os.getcwd(), \"train_bert_synthetic.ipynb\"\n",
    ")\n",
    "excluded_all_caps_params = {k for k in locals().keys() if k.isupper()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a63e06f-5fd8-46bf-8135-8ee47f219348",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Data parameters\n",
    "# DATASET_NAME = \"FirstTokenRepeatedOnce\"\n",
    "# DATASET_NAME = \"Contains1FirstToken\"\n",
    "# DATASET_NAME = \"FirstTokenRepeatedOnceImmediately\"\n",
    "# DATASET_NAME, DATASET_KWARGS_IDENTIFIABLE = \"ContainsTokenSet\", {\n",
    "#     \"num_points\": 1000,\n",
    "#     \"token_set\": [1, 2, 3],\n",
    "# }\n",
    "# DATASET_NAME, DATASET_KWARGS_IDENTIFIABLE = \"Contains1\", {\"num_points\": 20000}\n",
    "# DATASET_NAME, DATASET_KWARGS_IDENTIFIABLE = \"FirstTokenRepeatedImmediately\", {\n",
    "#     \"num_points\": 20000\n",
    "# }\n",
    "# DATASET_NAME, DATASET_KWARGS_IDENTIFIABLE = \"FirstTokenRepeatedLast\", {\n",
    "#     \"num_points\": 20000\n",
    "# }\n",
    "# DATASET_NAME, DATASET_KWARGS_IDENTIFIABLE = \"AdjacentDuplicate\", {\"num_points\": 20000}\n",
    "DATASET_NAME, DATASET_KWARGS_IDENTIFIABLE = \"BinCountOnes\", {\n",
    "    \"num_points\": 1200,\n",
    "    \"num_classes\": 2,\n",
    "    \"seq_len\": 64,\n",
    "}\n",
    "DATASET_KWARGS = {}\n",
    "SEED = 0\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 10\n",
    "VAL_BATCH_SIZE = 10\n",
    "\n",
    "MAX_TEST_SET_SIZE = 200\n",
    "MAX_VAL_SET_SIZE = 200\n",
    "MAX_VAL_SET_SIZE = 200\n",
    "\n",
    "SAMPLING_FRACTION = 1.0\n",
    "\n",
    "# Model parameters\n",
    "TRAIN_TORCH = True\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "L1_WEIGHT = 0.0\n",
    "L2_WEIGHT = 0.0\n",
    "DROPOUT_PROB = 0\n",
    "NUM_ATTENTION_HEADS = 4\n",
    "NUM_HIDDEN_LAYERS = 1\n",
    "# NUM_HIDDEN_LAYERS = 2\n",
    "ATTENTION_PROBS_DROPOUT_PROB = 0\n",
    "# ATTENTION_PROBS_DROPOUT_PROB = 0.2\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "OVERWRITE_MODEL = True\n",
    "MODEL_CONFIG_PATH = \"../brunoflow/models/bert/config-toy.json\"\n",
    "VALIDATE_DURING_TRAINING = False\n",
    "EPOCHS_FOR_VALIDATION = [0, 1, 2, 3, 4, 5, 10, 20, 30]\n",
    "\n",
    "# Analysis parameters\n",
    "COMPUTE_ENTROPY = True\n",
    "\n",
    "# Run parameters\n",
    "PM_RUN_ID = \"run_id\"\n",
    "PROJECT_NAME = \"bauer-bert-synthetic\"\n",
    "# GROUP_NAME = \"trial\"\n",
    "# GROUP_NAME = None\n",
    "\n",
    "TAGS = [\"train_torch\", \"trial\", \"bincountones\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a10f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dir: data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0\n",
      "Model dir: data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/models/Bert-hs32-numheads4-bs32-lr0.001-n30\n"
     ]
    }
   ],
   "source": [
    "# Construct dataset and data ids\n",
    "dataset = getattr(sys.modules[__name__], DATASET_NAME)(\n",
    "    **{**DATASET_KWARGS_IDENTIFIABLE, **DATASET_KWARGS}\n",
    ")\n",
    "data_id = f\"{dataset.get_name()}\"\n",
    "data_dir = os.path.join(\"data\", DATASET_NAME, data_id, f\"{SAMPLING_FRACTION}-{SEED}\")\n",
    "input_dir = os.path.join(data_dir, \"inputs\")\n",
    "train_data_path = os.path.join(input_dir, \"train.pt\")\n",
    "val_data_path = os.path.join(input_dir, \"val.pt\")\n",
    "val_data_df_path = os.path.join(input_dir, \"val_data.csv\")\n",
    "test_data_path = os.path.join(input_dir, \"test.pt\")\n",
    "\n",
    "# Construct model id\n",
    "model_id = f\"Bert-hs{HIDDEN_SIZE}-numheads{NUM_ATTENTION_HEADS}-bs{BATCH_SIZE}-lr{LEARNING_RATE}-n{NUM_EPOCHS}\"\n",
    "model_id += f\"-l1_weight{L1_WEIGHT}\" if L1_WEIGHT != 0 else \"\"\n",
    "model_id += f\"-l2_weight{L2_WEIGHT}\" if L2_WEIGHT != 0 else \"\"\n",
    "model_id += f\"-dropoutprob{DROPOUT_PROB}\" if DROPOUT_PROB != 0 else \"\"\n",
    "\n",
    "model_dir = os.path.join(data_dir, \"models\", model_id)\n",
    "\n",
    "print(f\"Data dir: {data_dir}\")\n",
    "print(f\"Model dir: {model_dir}\")\n",
    "\n",
    "# Construct model kwargs\n",
    "model_config_kwargs = dict(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "    attention_probs_dropout_prob=ATTENTION_PROBS_DROPOUT_PROB,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    ")\n",
    "\n",
    "if DATASET_NAME == \"BinCountOnes\":\n",
    "    model_config_kwargs[\n",
    "        \"num_labels\"\n",
    "    ] = (\n",
    "        dataset.seq_len\n",
    "    )  # set the num_labels to the max possible (even when num_classes is lower) so that the models have same number of connections regardless of the number of classes.\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b8d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_log = {\n",
    "    k: v\n",
    "    for k, v in locals().items()\n",
    "    if k.isupper() and k not in excluded_all_caps_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f20d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdu\u001b[0m (\u001b[33methz-rycolab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "wandb: ERROR Failed to sample metric: Not Supported\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kevin/code/rycolab/interpreta-bauer-ly/wandb/run-20230512_143732-13ho77e3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ethz-rycolab/bauer-bert-synthetic/runs/13ho77e3\" target=\"_blank\">snowy-durian-45</a></strong> to <a href=\"https://wandb.ai/ethz-rycolab/bauer-bert-synthetic\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATASET_NAME': 'BinCountOnes', 'DATASET_KWARGS_IDENTIFIABLE': {'num_points': 1200, 'num_classes': 2, 'seq_len': 64}, 'DATASET_KWARGS': {}, 'SEED': 0, 'BATCH_SIZE': 32, 'TEST_BATCH_SIZE': 10, 'VAL_BATCH_SIZE': 10, 'MAX_TEST_SET_SIZE': 200, 'MAX_VAL_SET_SIZE': 200, 'SAMPLING_FRACTION': 1.0, 'TRAIN_TORCH': True, 'HIDDEN_SIZE': 32, 'NUM_EPOCHS': 30, 'LEARNING_RATE': 0.001, 'L1_WEIGHT': 0.0, 'L2_WEIGHT': 0.0, 'DROPOUT_PROB': 0, 'NUM_ATTENTION_HEADS': 4, 'NUM_HIDDEN_LAYERS': 1, 'ATTENTION_PROBS_DROPOUT_PROB': 0, 'MAX_POSITION_EMBEDDINGS': 512, 'OVERWRITE_MODEL': True, 'MODEL_CONFIG_PATH': '../brunoflow/models/bert/config-toy.json', 'VALIDATE_DURING_TRAINING': False, 'EPOCHS_FOR_VALIDATION': [0, 1, 2, 3, 4, 5, 10, 20, 30], 'COMPUTE_ENTROPY': True, 'PM_RUN_ID': 'run_id', 'PROJECT_NAME': 'bauer-bert-synthetic', 'TAGS': ['train_torch', 'trial', 'bincountones']}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    # group=GROUP_NAME,\n",
    "    config=params_to_log,\n",
    "    tags=TAGS,\n",
    ")\n",
    "print(dict(wandb.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5914462",
   "metadata": {},
   "source": [
    "### Data Retrieval and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5f3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_subset_to_df(torch_data: torch.utils.data.dataset.Subset):\n",
    "    return [tuple(el.numpy().tolist() for el in row) for row in torch_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c23e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached train and test sets from data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/inputs/train.pt and data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/inputs/test.pt.\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    os.path.isfile(train_data_path)\n",
    "    and os.path.isfile(val_data_path)\n",
    "    and os.path.isfile(test_data_path)\n",
    "):\n",
    "    print(\n",
    "        f\"Loading cached train and test sets from {train_data_path} and {test_data_path}.\"\n",
    "    )\n",
    "    train_data = torch.load(train_data_path)\n",
    "    val_data = torch.load(val_data_path)\n",
    "    test_data = torch.load(test_data_path)\n",
    "else:\n",
    "    train_data = dataset.get_train_data()\n",
    "    val_data = dataset.get_val_data()\n",
    "    test_data = dataset.get_test_data()\n",
    "\n",
    "    # Save the dataset\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    torch.save(train_data, train_data_path)\n",
    "    torch.save(val_data, val_data_path)\n",
    "    torch.save(test_data, test_data_path)\n",
    "\n",
    "    val_data_df = pd.DataFrame(\n",
    "        torch_subset_to_df(val_data), columns=[\"sentence\", \"label\"]\n",
    "    )\n",
    "    val_data_df.to_csv(val_data_df_path)\n",
    "\n",
    "train_kwargs = {\"batch_size\": BATCH_SIZE}\n",
    "val_kwargs = {\"batch_size\": VAL_BATCH_SIZE}\n",
    "test_kwargs = {\"batch_size\": TEST_BATCH_SIZE}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **train_kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, **val_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80818ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/inputs/val_data.csv data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/models/Bert-hs32-numheads4-bs32-lr0.001-n30\n"
     ]
    }
   ],
   "source": [
    "print(val_data_df_path, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7faaff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/inputs)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging datasets to w&b run <wandb.sdk.wandb_run.Run object at 0x7f54c87d1eb0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7f54c85c7a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After loading/preprocessing your dataset, log it as an artifact to W&B\n",
    "print(f\"Logging datasets to w&b run {wandb.run}.\")\n",
    "artifact = wandb.Artifact(name=data_id, type=\"dataset\")\n",
    "artifact.add_dir(local_path=input_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d675190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[14, 11,  9,  4,  1,  1,  1,  4,  9, 17,  1, 15, 15, 14, 17,  8,  1,  3,\n",
       "          12,  8, 12,  5, 15,  1,  1, 13, 13, 10,  1,  1, 16,  1,  1, 16,  9,  1,\n",
       "          11,  9,  1, 10,  2, 17,  1, 18, 18,  1,  1,  8,  6,  3,  1,  9, 19,  1,\n",
       "          12,  1,  1,  2,  3, 15,  9,  9,  1,  1],\n",
       "         [ 1,  1,  1,  3,  6, 18,  1, 16,  1,  1,  1,  2,  1, 12,  1,  8,  9,  1,\n",
       "           1, 16,  1, 12, 19,  5,  8,  1, 14,  1,  5,  9,  2,  1,  1,  1,  1, 19,\n",
       "           6, 12,  1, 14,  2,  4,  2,  1, 13,  1,  7,  1,  1,  1,  1,  8,  2, 10,\n",
       "           1, 10,  1,  7, 12, 14,  1, 17,  1,  1],\n",
       "         [ 1, 11,  1,  9,  1,  5,  1, 13, 13, 14,  1, 14,  1,  1,  1,  1,  2,  1,\n",
       "           7,  1,  3,  1,  1,  8,  4,  1, 15,  1,  1, 17,  1,  1,  1, 14, 10,  5,\n",
       "           1,  1,  5, 15,  1,  1,  1,  9,  1,  1,  1,  1, 14,  8,  5,  1,  1, 18,\n",
       "           1,  1, 16,  1,  1,  1,  7,  1,  1,  1],\n",
       "         [ 1,  1, 18,  3,  1, 18,  1,  3, 17, 13,  2,  7,  1, 13,  7,  7,  1,  1,\n",
       "          19,  7, 13, 10,  5,  1,  1, 11,  1,  1,  6, 13,  9, 17, 12,  1,  1,  1,\n",
       "           8, 17,  3, 10,  4,  2, 19,  1, 11,  1, 12, 16, 19,  1,  1,  1,  6,  1,\n",
       "           1,  4, 11, 13, 15,  1, 17,  1,  1,  1],\n",
       "         [ 8,  1,  1,  1,  1, 11,  1,  1,  5, 18, 16,  1, 18,  9,  5,  1, 12, 19,\n",
       "           1,  4, 15, 12,  6, 17,  1,  1,  1,  1,  1,  8,  1, 11,  1,  1, 17,  2,\n",
       "          15,  1, 10,  1,  1,  1,  1,  1,  1,  1, 19,  1, 16,  1, 17, 11, 17,  1,\n",
       "          10,  1, 17, 16, 19,  1,  1,  1,  1,  1],\n",
       "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "           1,  1,  1,  1,  1, 17, 11,  1,  1,  1,  1,  9,  1,  1,  1,  1,  1, 10,\n",
       "           1,  1, 14,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 14,  1,  1,  1,  1,\n",
       "           1,  1,  1, 17,  1,  1,  1,  1,  1,  1],\n",
       "         [ 1,  3,  1,  1, 13, 14,  7,  1, 12,  1,  2,  9,  3, 17,  1,  5,  1, 15,\n",
       "          11,  1,  1,  8,  1, 10,  1,  1,  1,  1,  7,  1, 14, 19,  1,  6,  1,  8,\n",
       "           1,  1, 13,  1,  7,  1,  1,  1, 12,  1,  1,  1, 14,  1,  6,  1,  1,  1,\n",
       "           1,  1,  1,  1,  1, 10,  1,  1,  6,  1],\n",
       "         [ 5,  1,  1,  1, 10,  1,  1,  1,  1,  1,  1, 16,  1,  7,  1,  1,  1, 10,\n",
       "           1,  1,  1,  5,  1,  1,  1,  1, 15,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 17,  1, 11,  1, 18, 17,  1,\n",
       "           1,  1,  1,  1,  1,  1, 11, 19,  1,  1],\n",
       "         [16,  3, 18,  5, 13,  3,  7, 18,  9, 11, 10, 19, 18,  2, 15, 11, 11, 11,\n",
       "          19, 13,  7, 19, 19,  3,  1,  4, 13,  3,  7, 10, 14,  7, 11,  9, 17, 17,\n",
       "          16,  8, 16,  1, 11, 14, 15, 13, 14,  7, 10, 11, 17, 11, 18, 13, 19, 14,\n",
       "          11, 14, 12,  9, 19, 19,  1, 11,  2, 16],\n",
       "         [ 1, 18, 11,  1,  1,  1, 14,  1,  1,  1,  1,  1,  1, 16, 12,  1,  1,  1,\n",
       "          15,  3, 17, 13,  2, 15,  1,  1,  1,  1,  1, 17,  1, 13,  6,  1,  1,  1,\n",
       "           1, 10, 18,  1,  1,  1, 12,  1, 16,  1,  1,  1, 17, 11,  2, 14,  1,  1,\n",
       "          13,  1,  1,  1,  1, 13,  1,  4,  1,  1]]),\n",
       " tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "example_data, example_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73031b1-c43a-4d41-b6b8-70e80f7041ad",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d418e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bf_model(config, model_path):\n",
    "    \"\"\"Load a BF model from a saved model path (either torch or bf)\"\"\"\n",
    "    bf_model = BfBertForSequenceClassification(config)\n",
    "    bf_model.load_state_dict(torch.load(model_path))\n",
    "    return bf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2b9e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(\n",
    "    model,\n",
    "    optimizer,\n",
    "    val_loader,\n",
    "    epoch,\n",
    "    batch,\n",
    "    compute_entropy=False,\n",
    "    max_val_set_size=None,\n",
    "):\n",
    "    # Initialize accumulators (bc we will need to break validation into many batches for computing entropy, etc)\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_test_points = 0\n",
    "    total_entropy = 0\n",
    "    total_grad = 0\n",
    "    total_abs_val_grad = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Loop through each batch in the val_loader\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # Apply model to inputs\n",
    "        bert_outputs = model(inputs, labels=labels)\n",
    "        logits = bert_outputs.logits\n",
    "        num_correct_in_batch = sum(np.argmax(logits.val, axis=1) == labels)\n",
    "        loss: bf.Node = cross_entropy_loss(logits, labels, reduction=\"sum\")\n",
    "\n",
    "        if compute_entropy:\n",
    "            # Compute and accumulate entropy, grad, abs_val_grad for the batch in the validation set\n",
    "            optimizer.zero_gradients()\n",
    "            model.train()\n",
    "            loss.backprop(values_to_compute=(\"abs_val_grad\", \"entropy\", \"grad\"))\n",
    "            model.eval()\n",
    "            entropy_per_example_per_token: np.ndarray = gather_entropies_of_input_ids(\n",
    "                model=model, input_ids=inputs\n",
    "            )  # shape: (bs, seq_len)\n",
    "\n",
    "            abs_val_grads_per_example_per_token = gather_abs_grad_of_input_ids(\n",
    "                model, inputs\n",
    "            )  # shape=(len(input_ids), hidden_sz)\n",
    "            grads_per_example_per_token = gather_grad_of_input_ids(\n",
    "                model, inputs\n",
    "            )  # shape=(len(input_ids), hidden_sz)\n",
    "\n",
    "            entropy = np.sum(entropy_per_example_per_token)  # shape: ()\n",
    "            abs_val_grad = np.sum(abs_val_grads_per_example_per_token)\n",
    "            grad = np.sum(grads_per_example_per_token)\n",
    "\n",
    "            total_entropy += entropy\n",
    "            total_abs_val_grad += abs_val_grad\n",
    "            total_grad += grad\n",
    "\n",
    "        total_loss += loss\n",
    "        total_correct += num_correct_in_batch\n",
    "        total_test_points += len(labels)\n",
    "        if max_val_set_size is not None and total_test_points > max_val_set_size:\n",
    "            break\n",
    "\n",
    "    # Compute mean statistics across entire validation cohort\n",
    "    accuracy = total_correct / total_test_points\n",
    "    mean_entropy = total_entropy / total_test_points if compute_entropy else None\n",
    "    mean_grad = total_grad / total_test_points if compute_entropy else None\n",
    "    mean_abs_val_grad = (\n",
    "        total_abs_val_grad / total_test_points if compute_entropy else None\n",
    "    )\n",
    "    mean_loss = total_loss / total_test_points\n",
    "\n",
    "    val_metrics = {\n",
    "        \"val\": {\n",
    "            \"loss\": mean_loss.val,\n",
    "            \"grad\": mean_grad,\n",
    "            \"abs_grad\": mean_abs_val_grad,\n",
    "            \"entropy\": mean_entropy,\n",
    "            \"epoch\": epoch,\n",
    "            \"batch\": batch,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.log(val_metrics)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f97139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_model(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    model_dir: str,\n",
    "    overwrite_model=False,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    l1_weight=0,\n",
    "    l2_weight=0,\n",
    "    validation_params_and_loader=dict(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a torch model.\n",
    "    validation_params_and_loader is a parameter that should be an empty dict if not doing validation.\n",
    "    \"\"\"\n",
    "    required_val_keys = {\n",
    "        \"val_loader\",\n",
    "        \"compute_entropy\",\n",
    "        \"epochs_for_validation\",\n",
    "        \"max_val_set_size\",\n",
    "    }\n",
    "    if validation_params_and_loader and not required_val_keys.issubset(\n",
    "        set(validation_params_and_loader.keys())\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Included a nonempty validation_params_and_loader with keys {set(validation_params_and_loader.keys())}, but didn't contain all of the required keys: {required_val_keys}.\"\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "\n",
    "    # Load model if already trained\n",
    "    if os.path.isfile(model_path):\n",
    "        if overwrite_model:\n",
    "            print(f\"Retraining and overwriting model at {model_path}.\")\n",
    "        else:\n",
    "            print(f\"Loading trained model from {model_path}.\")\n",
    "            model = torch.load(model_path)\n",
    "            # Wow there's some DANGER here of the optimizer being attached to a different model than the one being loaded here...oof\n",
    "\n",
    "            return model\n",
    "\n",
    "    # Train the model\n",
    "    losses = dict()\n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            bert_outputs = model(inputs, labels=labels)\n",
    "            unregularized_loss = bert_outputs.loss\n",
    "            # unregularized_loss = cross_entropy_loss(outputs, labels)\n",
    "            loss = unregularized_loss + regularize(\n",
    "                model=model, l1_weight=l1_weight, l2_weight=l2_weight\n",
    "            )\n",
    "\n",
    "            # Zero out all semiring values (e.g. gradients, entropy, etc) from the previous minibatch to compute the gradients and entropy correctly.\n",
    "            optimizer.zero_grad()\n",
    "            unregularized_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if wandb.run is not None:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train\": {\n",
    "                            \"unregularized_loss\": unregularized_loss,\n",
    "                            \"epoch\": epoch,\n",
    "                            \"batch\": i,\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i}/{n_total_steps}], \"\n",
    "                    f\"Cross-Entropy Loss: {unregularized_loss:.4f}, \"\n",
    "                )\n",
    "\n",
    "                train_step_num = i + n_total_steps * epoch\n",
    "                losses[train_step_num] = loss\n",
    "\n",
    "                # Construct and save entropies and losses to CSV\n",
    "                losses_df = pd.DataFrame([losses]).melt(\n",
    "                    var_name=\"steps\", value_name=\"loss\"\n",
    "                )\n",
    "                losses_df.to_csv(os.path.join(model_dir, \"losses.csv\"))\n",
    "\n",
    "            if (\n",
    "                validation_params_and_loader\n",
    "                and epoch in validation_params_and_loader[\"epochs_for_validation\"]\n",
    "                and i == 0\n",
    "            ):\n",
    "                print(f\"Starting validation at epoch {epoch}\")\n",
    "                # Note: if you add/change the keys of validation_params_and_loader, you'll need to change the assert at the start of this fct\n",
    "                val_loader = validation_params_and_loader[\"val_loader\"]\n",
    "                compute_entropy = validation_params_and_loader[\"compute_entropy\"]\n",
    "                max_val_set_size = validation_params_and_loader[\"max_val_set_size\"]\n",
    "                # Run validation\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                bf_model = load_bf_model(model.config, model_path=model_path)\n",
    "                bf_optimizer = Adam(bf_model.parameters(), step_size=LEARNING_RATE)\n",
    "                val_metrics = validation(\n",
    "                    model=bf_model,\n",
    "                    optimizer=bf_optimizer,\n",
    "                    val_loader=val_loader,\n",
    "                    epoch=epoch,\n",
    "                    batch=i,\n",
    "                    compute_entropy=compute_entropy,\n",
    "                    max_val_set_size=max_val_set_size,\n",
    "                )\n",
    "                print(\n",
    "                    f\"VALIDATION @ Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i}/{n_total_steps}], \"\n",
    "                    # f'Total Loss: {regularized_loss.val:.4f}, '\n",
    "                    # f\"Unregularized Cross-Entropy Loss: {unregularized_loss.val:.4f}, \"\n",
    "                    f\"Accuracy: {val_metrics['val']['accuracy']:.4f}, \"\n",
    "                    f\"Cross-Entropy Loss: {val_metrics['val']['loss']:.4f}, \"\n",
    "                    f\"Entropy: {val_metrics['val']['entropy']:.4f}\"\n",
    "                )\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Log to W&B\n",
    "    if wandb.run is not None:\n",
    "        print(f\"Logging model to w&b run {wandb.run}.\")\n",
    "        artifact = wandb.Artifact(name=\"model\", type=\"model\")\n",
    "        artifact.add_file(local_path=model_path)\n",
    "        run.log_artifact(artifact)\n",
    "\n",
    "    return model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1982a-c924-45bb-87ec-14d67db0d8c6",
   "metadata": {},
   "source": [
    "### Define evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf416a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    model_dir,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    compute_entropy=False,\n",
    "    overwrite_metrics=False,\n",
    "    max_test_set_size: int = None,\n",
    "):\n",
    "    model.eval()\n",
    "    metrics_path = os.path.join(model_dir, \"metrics.csv\")\n",
    "    if os.path.isfile(metrics_path):\n",
    "        if overwrite_metrics:\n",
    "            print(f\"Recomputing and overwriting metrics at {metrics_path}.\")\n",
    "        else:\n",
    "            print(f\"Loading precomputed metrics from {metrics_path}.\")\n",
    "            return pd.read_csv(metrics_path)\n",
    "\n",
    "    metrics = dict()\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_test_points = 0\n",
    "    total_entropy = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.numpy()\n",
    "        labels = labels.numpy()\n",
    "        bert_outputs = model(inputs, labels=labels)\n",
    "        logits = bert_outputs.logits\n",
    "        num_correct_in_batch = sum(np.argmax(logits.val, axis=1) == labels)\n",
    "        loss: bf.Node = cross_entropy_loss(logits, labels, reduction=\"sum\")\n",
    "\n",
    "        if compute_entropy:\n",
    "            optimizer.zero_gradients()\n",
    "            model.train()\n",
    "            loss.backprop(values_to_compute=(\"abs_val_grad\", \"entropy\"))\n",
    "            model.eval()\n",
    "            entropy_per_example_per_token: np.ndarray = gather_entropies_of_input_ids(\n",
    "                model=model, input_ids=inputs\n",
    "            )  # shape: (bs, seq_len)\n",
    "            entropy = np.sum(entropy_per_example_per_token)  # shape: ()\n",
    "            total_entropy += entropy\n",
    "\n",
    "        total_loss += loss\n",
    "        total_correct += num_correct_in_batch\n",
    "        total_test_points += len(labels)\n",
    "        if max_test_set_size is not None and total_test_points > max_test_set_size:\n",
    "            break\n",
    "\n",
    "    accuracy = total_correct / total_test_points\n",
    "    mean_entropy = total_entropy / total_test_points if compute_entropy else None\n",
    "    loss = total_loss / total_test_points\n",
    "\n",
    "    metrics = {\"accuracy\": accuracy, \"loss\": loss.val, \"entropy\": mean_entropy}\n",
    "    metrics_df = pd.DataFrame.from_dict([metrics])\n",
    "\n",
    "    metrics_df.to_csv(os.path.join(model_dir, \"metrics.csv\"))\n",
    "\n",
    "    # Log to W&B\n",
    "    if wandb.run is not None:\n",
    "        print(f\"Logging test metrics to w&b run {wandb.run}.\")\n",
    "        wandb.log({\"test\": metrics})\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be668f7e-005f-4850-9052-7e544a0e51a0",
   "metadata": {},
   "source": [
    "### Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a105830",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dir = os.path.join(model_dir, \"mem_profiles\")\n",
    "os.makedirs(profile_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "026b6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.012 seconds\n",
      "WARNING: TODO(KD) - when this is used, write some tests for this!\n",
      "WARNING: TODO(KD) - when this is used, write some tests for this!\n",
      "WARNING: TODO(KD) - when this is used, write some tests for this!\n",
      "Retraining and overwriting model at data/BinCountOnes/BinCountOnes-num_classes2-seqlen64-num_points1200-vs20/1.0-0/models/Bert-hs32-numheads4-bs32-lr0.001-n30/model.pt.\n",
      "Epoch [1/30], Step [0/27], Cross-Entropy Loss: 4.1664, \n",
      "Epoch [2/30], Step [0/27], Cross-Entropy Loss: 2.8623, \n",
      "Epoch [3/30], Step [0/27], Cross-Entropy Loss: 1.1996, \n",
      "Epoch [4/30], Step [0/27], Cross-Entropy Loss: 0.8474, \n",
      "Epoch [5/30], Step [0/27], Cross-Entropy Loss: 0.7770, \n",
      "Epoch [6/30], Step [0/27], Cross-Entropy Loss: 0.7501, \n",
      "Epoch [7/30], Step [0/27], Cross-Entropy Loss: 0.7357, \n",
      "Epoch [8/30], Step [0/27], Cross-Entropy Loss: 0.7267, \n",
      "Epoch [9/30], Step [0/27], Cross-Entropy Loss: 0.7205, \n",
      "Epoch [10/30], Step [0/27], Cross-Entropy Loss: 0.7161, \n",
      "Epoch [11/30], Step [0/27], Cross-Entropy Loss: 0.7128, \n",
      "Epoch [12/30], Step [0/27], Cross-Entropy Loss: 0.7102, \n",
      "Epoch [13/30], Step [0/27], Cross-Entropy Loss: 0.7081, \n",
      "Epoch [14/30], Step [0/27], Cross-Entropy Loss: 0.7065, \n",
      "Epoch [15/30], Step [0/27], Cross-Entropy Loss: 0.7051, \n",
      "Epoch [16/30], Step [0/27], Cross-Entropy Loss: 0.7040, \n",
      "Epoch [17/30], Step [0/27], Cross-Entropy Loss: 0.7030, \n",
      "Epoch [18/30], Step [0/27], Cross-Entropy Loss: 0.7022, \n",
      "Epoch [19/30], Step [0/27], Cross-Entropy Loss: 0.7014, \n",
      "Epoch [20/30], Step [0/27], Cross-Entropy Loss: 0.7008, \n",
      "Epoch [21/30], Step [0/27], Cross-Entropy Loss: 0.7002, \n",
      "Epoch [22/30], Step [0/27], Cross-Entropy Loss: 0.6998, \n",
      "Epoch [23/30], Step [0/27], Cross-Entropy Loss: 0.6993, \n",
      "Epoch [24/30], Step [0/27], Cross-Entropy Loss: 0.6989, \n",
      "Epoch [25/30], Step [0/27], Cross-Entropy Loss: 0.6986, \n",
      "Epoch [26/30], Step [0/27], Cross-Entropy Loss: 0.6983, \n",
      "Epoch [27/30], Step [0/27], Cross-Entropy Loss: 0.6980, \n",
      "Epoch [28/30], Step [0/27], Cross-Entropy Loss: 0.6977, \n",
      "Epoch [29/30], Step [0/27], Cross-Entropy Loss: 0.6975, \n",
      "Epoch [30/30], Step [0/27], Cross-Entropy Loss: 0.6973, \n",
      "Logging model to w&b run <wandb.sdk.wandb_run.Run object at 0x7f54c87d1eb0>.\n",
      "Time: 5.915 seconds\n",
      "WARNING: TODO(KD) - when this is used, write some tests for this!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "with catchtime() as t:\n",
    "    config = BertConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path=MODEL_CONFIG_PATH,\n",
    "        **model_config_kwargs,\n",
    "    )\n",
    "    config.to_json_file(os.path.join(model_dir, \"config.json\"))\n",
    "\n",
    "    model = (\n",
    "        BfBertForSequenceClassification(config=config)\n",
    "        if not TRAIN_TORCH\n",
    "        else BertForSequenceClassification(config=config)\n",
    "    )\n",
    "\n",
    "\n",
    "init_model_time = t.time\n",
    "init_model_nvidia_mem_used = gpu_memory_usage()\n",
    "try:\n",
    "    jax.profiler.save_device_memory_profile(\n",
    "        os.path.join(profile_dir, f\"toy_bert_init.prof\")\n",
    "    )\n",
    "except:\n",
    "    print(\"Can't jax profile because no jax\")\n",
    "\n",
    "# Initialize optimizer\n",
    "\n",
    "# Train model\n",
    "with catchtime() as t:\n",
    "    if TRAIN_TORCH:\n",
    "        validation_params_and_loader = {\n",
    "            \"val_loader\": val_loader,\n",
    "            \"compute_entropy\": COMPUTE_ENTROPY,\n",
    "            \"max_val_set_size\": MAX_VAL_SET_SIZE,\n",
    "            \"epochs_for_validation\": EPOCHS_FOR_VALIDATION,\n",
    "        }\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        model, model_path = train_torch_model(\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_loader,\n",
    "            model_dir,\n",
    "            overwrite_model=OVERWRITE_MODEL,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            l1_weight=L1_WEIGHT,\n",
    "            l2_weight=L2_WEIGHT,\n",
    "            validation_params_and_loader=validation_params_and_loader\n",
    "            if VALIDATE_DURING_TRAINING\n",
    "            else dict(),\n",
    "        )\n",
    "    else:\n",
    "        optimizer = Adam(model.parameters(), step_size=LEARNING_RATE)\n",
    "        model, model_path = train_model(\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_loader,\n",
    "            model_dir,\n",
    "            overwrite_model=OVERWRITE_MODEL,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            l1_weight=L1_WEIGHT,\n",
    "            l2_weight=L2_WEIGHT,\n",
    "        )\n",
    "train_model_time = t.time\n",
    "train_model_nvidia_mem_used = gpu_memory_usage()\n",
    "try:\n",
    "    jax.profiler.save_device_memory_profile(\n",
    "        os.path.join(\n",
    "            profile_dir,\n",
    "            f\"toy_bert_train_model.prof\",\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    print(\"Can't jax profile because no jax\")\n",
    "\n",
    "# Evaluate model\n",
    "if TRAIN_TORCH:  # Load a BF model if model is currently a torch model\n",
    "    model = load_bf_model(config, model_path=model_path)\n",
    "    optimizer = Adam(model.parameters(), step_size=LEARNING_RATE)\n",
    "\n",
    "with catchtime() as t:\n",
    "    # validation(\n",
    "    #     model,\n",
    "    #     optimizer,\n",
    "    #     val_loader,\n",
    "    #     l1_weight=L1_WEIGHT,\n",
    "    #     l2_weight=L2_WEIGHT,\n",
    "    #     epoch=1,\n",
    "    #     batch=i,\n",
    "    # )\n",
    "    metrics = evaluate_model(\n",
    "        model,\n",
    "        model_dir,\n",
    "        test_loader,\n",
    "        optimizer=optimizer,\n",
    "        compute_entropy=COMPUTE_ENTROPY,\n",
    "        overwrite_metrics=OVERWRITE_MODEL,\n",
    "        max_test_set_size=MAX_TEST_SET_SIZE,\n",
    "    )\n",
    "eval_model_time = t.time\n",
    "eval_model_nvidia_mem_used = gpu_memory_usage()\n",
    "try:\n",
    "    jax.profiler.save_device_memory_profile(\n",
    "        os.path.join(\n",
    "            profile_dir,\n",
    "            f\"toy_bert_eval_model.prof\",\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    print(\"Can't jax profile because no jax\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log(\n",
    "    {\n",
    "        \"init_model_time\": init_model_time,\n",
    "        \"train_model_time\": train_model_time,\n",
    "        \"eval_model_time\": eval_model_time,\n",
    "        \"init_model_nvidia_mem_used\": init_model_nvidia_mem_used,\n",
    "        \"eval_model_nvidia_mem_used\": eval_model_nvidia_mem_used,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79746a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/BinCountOnes/BinCountOnes-num_classes2-seqlen12-num_points1200-vs20/1.0-0/models/Bert-hs32-numheads4-bs32-lr0.001-n5/mem_profiles)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging profiling info to w&b run <wandb.sdk.wandb_run.Run object at 0x7f37af69e8b0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7f369814b4c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Logging profiling info to w&b run {wandb.run}.\")\n",
    "artifact = wandb.Artifact(name=f\"{data_id}_profiles\", type=\"profiles\")\n",
    "artifact.add_dir(local_path=profile_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebc592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_model_time</td><td>▁</td></tr><tr><td>init_model_time</td><td>▁</td></tr><tr><td>train_model_time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_model_nvidia_mem_used</td><td>1881 MiB</td></tr><tr><td>eval_model_time</td><td>9.71724</td></tr><tr><td>init_model_nvidia_mem_used</td><td>87 MiB</td></tr><tr><td>init_model_time</td><td>0.00361</td></tr><tr><td>train_model_time</td><td>62.42895</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">betazoid-t-pol-307</strong>: <a href=\"https://wandb.ai/kdu/bauer-bert-synthetic/runs/1xd3bxn3\" target=\"_blank\">https://wandb.ai/kdu/bauer-bert-synthetic/runs/1xd3bxn3</a><br/>Synced 7 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230405_150901-1xd3bxn3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d12306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
